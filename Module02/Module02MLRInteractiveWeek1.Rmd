---
title: "TMA4315 Generalized linear models H2018"
subtitle: "Module 2: MULTIPLE LINEAR REGRESSION, interactive sessions"
author: "Mette Langaas, Department of Mathematical Sciences, NTNU -- with contributions from Oyvind Bakke and Ingeborg Hem" 
date: "30.08 and 06.09 [PL], 31.08 and 07.09 [IL]"
output:
 html_document:
   toc: true
   toc_float: true
   toc_depth: 2
   css: "../TMA4315RMarkdown.css"
  # pdf_document:
  #   toc: true
  #   toc_depth: 2
  #   keep_tex: yes
  # beamer_presentation:
  #   keep_tex: yes
  #   fig_caption: false
  #   latex_engine: xelatex
always_allow_html: yes
---

<!-- rmarkdown::render("/Users/mettela/Box Sync/NTNUgitcourses/NRkurs/NRkursC.Rmd","html_document") -->
<!-- rmarkdown::render("/Users/mettela/Box Sync/NTNUgitcourses/NRkurs/NRkursC.Rmd","beamer_presentation") -->
<!-- rmarkdown::render("/Users/mettela/Box Sync/NTNUgitcourses/NRkurs/NRkursC.Rmd","pdf_document") -->

```{r setup, include=FALSE}
library(formatR)
showsol<-FALSE
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=68),tidy=TRUE,warning=FALSE,error=FALSE,message=FALSE)#,results="hold"
knitr::opts_chunk$set(echo = TRUE)
```


# Interactive lectures- problem set first week

## Theoretical questions

### Problem 1

1. <span class="question">Write down the GLM way for the multiple linear regression model. Explain how it is different.</span>

2. <span class="question">Write down the likelihood and loglikelihood. Then define the score vector.</span>

3. <span class="question">What is the set of equations we solve to find parameter estimates? What if we could not find a closed form solution to our set of equations - what could we do then?</span>

4. <span class="question">Define the observed and the expected Fisher information matrix. What dimension does these matrices have? What can these matrices tell us?</span>

---

4. A core finding is $\hat \beta$. 
$$
\hat{\beta}=({\bf X}^T{\bf X})^{-1} {\bf X}^T {\bf Y}
$$

with $\hat{\beta}\sim N_{p}(\beta,\sigma^2({\bf X}^T{\bf X})^{-1})$.

<span class="question">Show that $\hat{\beta}$ has this distribution with the given mean and covariance matrix.</span>

<span class="question">What does this imply for the distribution of the $j$th element of $\hat{\beta}$? In particular, how can we calculate the variance of $\hat{\beta}_j$?</span>

5. <span class="question">Explain the difference between _error_ and _residual_.  What are the properties of the raw residuals? Why don't we want to use the raw residuals for model check? What is our solution to this?</span>

6. <span class="question">What is the theoretical intercept and slope of a QQ--plot based on a normal sample?</span>

**Hint: [QQ--plot as html](https://www.math.ntnu.no/emner/TMA4315/2017h/qq.html)**

---

## Interpretation and understanding

### Problem 2: Munich Rent Index data

Fit the regression model with first `rent` and then `rentsqm` as reponse and following covariates: `area`, `location` (dummy variable coding using location2 and location3), `bath`, `kitchen` and `cheating` (central heating).

```{r}
library(gamlss.data)
library(ggfortify)
?rent99

mod1 <- lm(rent ~ area + location + bath + kitchen + cheating, data = rent99)
mod2 <- lm(rentsqm ~ area + location + bath + kitchen + cheating, data = rent99)
autoplot(mod1, label.size = 2)
autoplot(mod2, label.size = 2)
```

1. <span class="question">Look at diagnostic plots for the two fits. Which response do you prefer?</span>

Concentrate on the response-model you choose for the rest of the tasks.

2. <span class="question">Explain what the parameter estimates mean in practice. In particular, what is the interpretation of the intercept?</span>

```{r}
summary(mod1)
summary(mod2)
```

<!-- 3. What if you wanted to recode the location variable (average-good-excellent) so that the good location is the reference level. How can you do that, and what would that mean for your fitted model? Hint: first define a location factor, and then use `relevel`. -->

3. <span class="question">Go through the summary printout and explain the parts you know now, and also observe the parts you don't know yet (on the agenda for next week?).</span>

Next week: more on inference on this data set.

---

### Problem 3: Simple vs. multiple regression

We look at a regression problem where both the response and the covariates are centered - that is, the mean of the response and the mean of each covariate is zero. We do this to avoid the intercept term, which makes things a bit more complicated.

1. <span class="question">In a design matrix (without an intercept column) orthogonal columns gives diagonal ${\bf X}^T {\bf X}$. What does that mean? How can we get orthogonal columns?</span>

2. <span class="question">If we have orthogonal columns, will then simple (only one covariate) and multiple estimated regression coefficients be different? Explain.</span>

3. <span class="question">What is multicollinearity? Is that a problem? Why (not)?</span>

---

### Problem 4: Dummy vs. effect coding in MLR

Background material for this task: [Categorical covariates - dummy and effect coding)(#categorical)

We will study a dataset where we want to model `income` as response and two unordered categorical covariates `gender`and `place` (location). 

```{r}
income <- c(300, 350, 370, 360, 400, 370, 420, 390,
            400,430,420, 410, 300, 320, 310, 305,
            350, 370, 340, 355,370, 380, 360, 365)
gender <- c(rep("Male", 12),rep("Female",12))
place <- rep(c(rep("A",4),rep("B",4),rep("C",4)),2)
data <- data.frame(income,gender=factor(gender,levels=c("Female","Male")),
                   place=factor(place,levels=c("A","B","C")))
```

1. First, describe the data set. 

```{r,warnings=FALSE}
library(GGally)
GGally::ggpairs(data)
```

2. <span class="question">Check out the `interaction.plot(data$gender,data$place,data$income)`. What does it show? Do we need an interaction term if we want to model a MLR with `income` as response?</span>

```{r}
interaction.plot(x.factor = data$gender, trace.factor = data$place, response = data$income, type = "l")
```

3. <span class="question">Check our `plot.design(income~place+gender, data = data)`. What does it show?</span>

```{r}
plot.design(income ~ place + gender, data = data)
```

4. First, use treatment contrast (dummy variable coding) and fit a MLR with `income` as response and `gender` and `place` as covariates. 

<span class="question">Explain what your model estimates mean. In particular, what is the interpretation of the intercept estimate?</span>

```{r}
mod3 <- lm(income ~ place + gender, data = data)
mod3
```

5. Now, turn to sum-zero contrast (effect coding). 

<span class="question">Explain what your model estimates mean. Now, what is the intercept estimate?</span>

<span class="question">Calculate the estimate for `place=C`.</span>

```{r}
mod4 <- lm(income ~ place + gender, data = data, contrasts = list(place = "contr.sum", gender = "contr.sum"))
mod4
model.matrix(mod4)
mean(income)
```

Next week we connect this to linear hypotheses and ANOVA.

---

### Problem 5: Interactions

This part of the module was marked "self-study". Go through this together in the group, and make sure that you understand.

### Problem 6: Simulations in R (optional)
(a version this problem was also given as recommended exercise in TMA4268 Statistical learning)

1. For simple linear regression, simulate at data set with homoscedastic errore and with heteroscedastic errors. Here is a suggestion of one solution.
Why this? To see how things looks when the model is correct and wrong. Look at the code and discuss what is done, and relate this to the plots of errors (which are usually unobserved) and plots of residuals.

```{r, eval=FALSE}
#Homoscedastic errors
n=1000
x=seq(-3,3,length=n)
beta0=-1
beta1=2
xbeta=beta0+beta1*x
sigma=1
e1=rnorm(n,mean=0,sd=sigma)
y1=xbeta+e1
ehat1=residuals(lm(y1~x))
plot(x,y1,pch=20)
abline(beta0,beta1,col=1)
plot(x,e1,pch=20)
abline(h=0,col=2)
plot(x,ehat1,pch=20)
abline(h=0,col=2)

#Heteroscedastic errors
sigma=(0.1+0.3*(x+3))^2
e2=rnorm(n,0,sd=sigma)
y2=xbeta+e2
ehat2=residuals(lm(y2~x))
plot(x,y2,pch=20)
abline(beta0,beta1,col=2)
plot(x,e2,pch=20)
abline(h=0,col=2)
plot(x,ehat2,pch=20)
abline(h=0,col=2)
```

2. All this fuss about raw, standardized and studentized residuals- does really matter in practice? Below is one example where the raw residuals are rather different from the standardized, but the standardized is identical to the studentized. Can you come up with a simuation model where the standardized and studentized are very different? Hint: what about at smaller sample size?

```{r, eval=FALSE}
n=1000
beta=matrix(c(0,1,1/2,1/3),ncol=1)
set.seed(123)
x1=rnorm(n,0,1); x2=rnorm(n,0,2); x3=rnorm(n,0,3)
X=cbind(rep(1,n),x1,x2,x3)
y=X%*%beta+rnorm(n,0,2)
fit=lm(y~x1+x2+x3)
yhat=predict(fit)
summary(fit)
ehat=residuals(fit); estand=rstandard(fit); estud=rstudent(fit)
plot(yhat,ehat,pch=20)
points(yhat,estand,pch=20,col=2)
#points(yhat,estud,pch=19,col=3)
```




# R packages

```{r, eval=FALSE}
install.packages(c("formatR",
                   "gamlss.data",
                   "tidyverse",
                   "ggplot2", 
                   "GGally", 
                   "Matrix",
                   "nortest",
                   "lmtest",
                   "wordcloud2",
                   "tm"))
```


# References and further reading

* Slightly different presentation (more focus on multivariate normal theory): [Slides and written material from TMA4267 Linear Statistical Models in 2017, Part 2:  Regression (by Mette Langaas).](https://www.math.ntnu.no/emner/TMA4267/2017v/TMA4267V2017Part2.pdf)
* And, same source, but now [Slides and written material from TMA4267 Linear Statistical Models in 2017, Part 3: Hypothesis testing and ANOVA] (http://www.math.ntnu.no/emner/TMA4267/2017v/TMA4267V2017Part3.pdf)


